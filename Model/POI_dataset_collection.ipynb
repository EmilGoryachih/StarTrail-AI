{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-17T09:34:12.422066Z",
     "start_time": "2025-06-17T09:34:09.674748Z"
    }
   },
   "source": [
    "!pip install requests pandas\n",
    "!pip install tqdm"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\emil1\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T11:14:04.062764Z",
     "start_time": "2025-06-17T09:34:12.434651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "OVERPASS_URL = \"http://overpass-api.de/api/interpreter\"\n",
    "WIKIPEDIA_API_URL = \"https://ru.wikipedia.org/w/api.php\"\n",
    "WIKIDATA_API_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "all_processed_pois = []\n",
    "\n",
    "def get_wikipedia_summary(title):\n",
    "    \"\"\"Получает краткое описание из Википедии по названию статьи.\"\"\"\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"query\",\n",
    "            \"format\": \"json\",\n",
    "            \"titles\": title,\n",
    "            \"prop\": \"extracts\",\n",
    "            \"exintro\": True,\n",
    "            \"explaintext\": True,\n",
    "            \"redirects\": 1\n",
    "        }\n",
    "        response = requests.get(WIKIPEDIA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        page_id = next(iter(data['query']['pages']))\n",
    "        if page_id != '-1':\n",
    "            return data['query']['pages'][page_id].get('extract', '')\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Таймаут при запросе к Википедии для {title}\")\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def get_wikidata_description(qid):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"wbgetentities\",\n",
    "            \"format\": \"json\",\n",
    "            \"ids\": qid,\n",
    "            \"props\": \"descriptions\",\n",
    "            \"languages\": \"ru|en\"\n",
    "        }\n",
    "        response = requests.get(WIKIDATA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        entity = data['entities'].get(qid)\n",
    "        if entity and 'descriptions' in entity:\n",
    "            if 'ru' in entity['descriptions']:\n",
    "                return entity['descriptions']['ru']['value']\n",
    "            elif 'en' in entity['descriptions']:\n",
    "                return entity['descriptions']['en']['value']\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Таймаут при запросе к Wikidata для {qid}\")\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        return \"\"\n",
    "\n",
    "def create_text_description(tags):\n",
    "    description_parts = []\n",
    "\n",
    "    if 'name' in tags:\n",
    "        description_parts.append(tags['name'])\n",
    "\n",
    "    tag_translations = {\n",
    "        'amenity': 'Тип',\n",
    "        'shop': 'Магазин',\n",
    "        'tourism': 'Туризм',\n",
    "        'leisure': 'Досуг',\n",
    "        'historic': 'Исторический объект',\n",
    "        'cuisine': 'Кухня'\n",
    "    }\n",
    "\n",
    "    for key, name in tag_translations.items():\n",
    "        if key in tags:\n",
    "            description_parts.append(f\"{name}: {tags[key].replace('_', ' ')}\")\n",
    "\n",
    "    if 'opening_hours' in tags:\n",
    "        description_parts.append(f\"Часы работы: {tags['opening_hours']}\")\n",
    "    if 'wheelchair' in tags and tags['wheelchair'] == 'yes':\n",
    "        description_parts.append(\"Доступно для инвалидных колясок\")\n",
    "    if 'internet_access' in tags and tags['internet_access'] == 'yes':\n",
    "        description_parts.append(\"Есть доступ в интернет\")\n",
    "    if 'phone' in tags:\n",
    "        description_parts.append(f\"Телефон: {tags['phone']}\")\n",
    "    if 'website' in tags:\n",
    "        description_parts.append(f\"Вебсайт: {tags['website']}\")\n",
    "\n",
    "    if 'description' in tags:\n",
    "        description_parts.append(f\"Описание: {tags['description']}\")\n",
    "\n",
    "    extra_description = \"\"\n",
    "    if 'wikipedia' in tags:\n",
    "        parts = tags['wikipedia'].split(':')\n",
    "        if len(parts) >= 2:\n",
    "            lang = parts[0]\n",
    "            title = parts[-1]\n",
    "            if lang == 'ru':\n",
    "                wiki_summary = get_wikipedia_summary(title)\n",
    "                if wiki_summary:\n",
    "                    extra_description = wiki_summary\n",
    "    elif 'wikidata' in tags:\n",
    "        wikidata_id = tags['wikidata']\n",
    "        wiki_description = get_wikidata_description(wikidata_id)\n",
    "        if wiki_description:\n",
    "            extra_description = wiki_description\n",
    "\n",
    "    if extra_description:\n",
    "        description_parts.append(extra_description)\n",
    "\n",
    "    return \". \".join(description_parts)\n",
    "\n",
    "def process_city_pois(city_name):\n",
    "    print(f\"\\n--- Начинаем обработку города: {city_name} ---\")\n",
    "    overpass_query = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    area[name=\"{city_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"tourism\"](area.searchArea);\n",
    "      way[\"tourism\"](area.searchArea);\n",
    "\n",
    "      node[\"leisure\"](area.searchArea);\n",
    "      way[\"leisure\"](area.searchArea);\n",
    "\n",
    "      node[\"historic\"](area.searchArea);\n",
    "      way[\"historic\"](area.searchArea);\n",
    "\n",
    "      node[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      way[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "\n",
    "      node[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "      way[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "\n",
    "      node[\"amenity\"~\"arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|community_centre|marketplace|atm|bank|clinic|hospital|pharmacy|post_office|police|fire_station|school|university|kindergarten|dentist|veterinary|parking|toilets|fountain|place_of_worship|courthouse|embassy|townhall|public_bath|sauna|stripclub|brothel\"](area.searchArea);\n",
    "      way[\"amenity\"~\"arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|community_centre|marketplace|place_of_worship\"](area.searchArea);\n",
    "      node[\"amenity\"~\"^(arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|marketplace|fountain|public_bath|sauna|stripclub|brothel)$\"](area.searchArea);\n",
    "      way[\"amenity\"~\"^(arts_centre|theatre|cinema|museum|library|nightclub|bar|restaurant|cafe|pub|food_court|marketplace|fountain|public_bath|sauna|stripclub|brothel)$\"](area.searchArea);\n",
    "\n",
    "      node[\"shop\"~\"^(mall|department_store|books|gift|souvenir|art|antiques|craft|boutique|jewelry|leather|music|shoes|toys|video)$\"](area.searchArea);\n",
    "      way[\"shop\"~\"^(mall|department_store|books|gift|souvenir|art|antiques|craft|boutique|jewelry|leather|music|shoes|toys|video)$\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    amenities_to_include = [\n",
    "        \"arts_centre\", \"theatre\", \"cinema\", \"museum\", \"library\", \"nightclub\", \"bar\",\n",
    "        \"restaurant\", \"cafe\", \"pub\", \"food_court\", \"marketplace\", \"fountain\",\n",
    "        \"public_bath\", \"sauna\", \"stripclub\", \"brothel\", \"casino\", \"ferry_terminal\",\n",
    "        \"attraction\", \"theme_park\", \"water_park\", \"zoo\", \"aquarium\", \"planetarium\",\n",
    "        \"gallery\", \"viewpoint\", \"observatory\"\n",
    "    ]\n",
    "    shops_to_include = [\n",
    "        \"mall\", \"department_store\", \"books\", \"gift\", \"souvenir\", \"art\", \"antiques\",\n",
    "        \"craft\", \"boutique\", \"jewelry\", \"leather\", \"music\", \"shoes\", \"toys\", \"video\",\n",
    "        \"kiosk\", \"convenience\"\n",
    "    ]\n",
    "\n",
    "    amenity_regex = \"|\".join(amenities_to_include)\n",
    "    shop_regex = \"|\".join(shops_to_include)\n",
    "\n",
    "    overpass_query_filtered = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    area[name=\"{city_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"tourism\"](area.searchArea);\n",
    "      way[\"tourism\"](area.searchArea);\n",
    "      node[\"leisure\"](area.searchArea);\n",
    "      way[\"leisure\"](area.searchArea);\n",
    "      node[\"historic\"](area.searchArea);\n",
    "      way[\"historic\"](area.searchArea);\n",
    "\n",
    "      node[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      way[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      node[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "      way[\"natural\"~\"park|wood|garden|beach|peak|water\"](area.searchArea);\n",
    "\n",
    "      node[\"amenity\"~\"^{amenity_regex}$\"](area.searchArea);\n",
    "      way[\"amenity\"~\"^{amenity_regex}$\"](area.searchArea);\n",
    "\n",
    "      node[\"shop\"~\"^{shop_regex}$\"](area.searchArea);\n",
    "      way[\"shop\"~\"^{shop_regex}$\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"🚀 Отправляем запрос к Overpass API для {city_name}...\")\n",
    "    try:\n",
    "        response = requests.get(OVERPASS_URL, params={'data': overpass_query_filtered}, timeout=180)\n",
    "        print(f\"✅ Запрос для {city_name} выполнен с кодом: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "        else:\n",
    "            print(f\"Ошибка выполнения запроса для {city_name}: {response.text}\")\n",
    "            data = {'elements': []}\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Таймаут при запросе к Overpass API для {city_name}\")\n",
    "        data = {'elements': []}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Ошибка запроса к Overpass API для {city_name}: {e}\")\n",
    "        data = {'elements': []}\n",
    "\n",
    "\n",
    "    current_city_pois = []\n",
    "    print(f\"⚙️ Начинаем обработку {len(data['elements'])} объектов для {city_name}...\")\n",
    "\n",
    "    for element in tqdm(data['elements'], desc=f\"Обработка POI в {city_name}\"):\n",
    "        if 'tags' in element:\n",
    "            tags = element['tags']\n",
    "\n",
    "            if 'name' not in tags:\n",
    "                continue\n",
    "\n",
    "            lat, lon = (0, 0)\n",
    "            if element['type'] == 'node':\n",
    "                lat = element.get('lat')\n",
    "                lon = element.get('lon')\n",
    "            elif 'center' in element:\n",
    "                lat = element['center'].get('lat')\n",
    "                lon = element['center'].get('lon')\n",
    "\n",
    "            if lat == 0 and lon == 0:\n",
    "                continue\n",
    "\n",
    "            text_description = create_text_description(tags)\n",
    "\n",
    "            current_city_pois.append({\n",
    "                'id': element['id'],\n",
    "                'type': element['type'],\n",
    "                'lat': lat,\n",
    "                'lon': lon,\n",
    "                'name': tags.get('name'),\n",
    "                'city': city_name,\n",
    "                'text_description': text_description,\n",
    "                'tags': tags\n",
    "            })\n",
    "\n",
    "        time.sleep(0.005) # 5 миллисекунд\n",
    "\n",
    "    print(f\"👍 Обработано {len(current_city_pois)} релевантных объектов с названиями для {city_name}.\")\n",
    "\n",
    "    all_processed_pois.extend(current_city_pois)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cities_to_process = [\"Казань\", \"Санкт-Петербург\", \"Москва\"]\n",
    "\n",
    "    for city in cities_to_process:\n",
    "        process_city_pois(city)\n",
    "        time.sleep(10)\n",
    "\n",
    "    print(\"\\n--- Обработка всех городов завершена! ---\")\n",
    "    print(f\"Всего собрано {len(all_processed_pois)} POI.\")\n",
    "\n",
    "    if all_processed_pois:\n",
    "        final_df = pd.DataFrame(all_processed_pois)\n",
    "        final_df.drop_duplicates(subset=['id'], inplace=True)\n",
    "        print(f\"Всего уникальных POI после удаления дубликатов: {len(final_df)}\")\n",
    "\n",
    "        output_filename = 'Dataset/poi_dataset_russia_filtered_enriched.csv'\n",
    "        final_df.to_csv(output_filename, index=False)\n",
    "        print(f\"💾 Окончательный датасет успешно сохранен в '{output_filename}'\")\n",
    "    else:\n",
    "        print(\"⚠️ Общий датасет пуст. Возможно, возникли ошибки при сборе данных.\")"
   ],
   "id": "ee5714ef775ca054",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Начинаем обработку города: Казань ---\n",
      "🚀 Отправляем запрос к Overpass API для Казань...\n",
      "✅ Запрос для Казань выполнен с кодом: 200\n",
      "⚙️ Начинаем обработку 73622 объектов для Казань...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка POI в Казань: 100%|██████████| 73622/73622 [06:29<00:00, 189.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👍 Обработано 3060 релевантных объектов с названиями для Казань.\n",
      "\n",
      "--- Начинаем обработку города: Санкт-Петербург ---\n",
      "🚀 Отправляем запрос к Overpass API для Санкт-Петербург...\n",
      "✅ Запрос для Санкт-Петербург выполнен с кодом: 200\n",
      "⚙️ Начинаем обработку 413806 объектов для Санкт-Петербург...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка POI в Санкт-Петербург: 100%|██████████| 413806/413806 [37:42<00:00, 182.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👍 Обработано 16858 релевантных объектов с названиями для Санкт-Петербург.\n",
      "\n",
      "--- Начинаем обработку города: Москва ---\n",
      "🚀 Отправляем запрос к Overpass API для Москва...\n",
      "✅ Запрос для Москва выполнен с кодом: 200\n",
      "⚙️ Начинаем обработку 608685 объектов для Москва...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка POI в Москва: 100%|██████████| 608685/608685 [54:07<00:00, 187.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👍 Обработано 21178 релевантных объектов с названиями для Москва.\n",
      "\n",
      "--- Обработка всех городов завершена! ---\n",
      "Всего собрано 41096 POI.\n",
      "Всего уникальных POI после удаления дубликатов: 41087\n",
      "💾 Окончательный датасет успешно сохранен в 'poi_dataset_russia_filtered_enriched.csv'\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-17T12:15:10.219522Z",
     "start_time": "2025-06-17T11:58:56.662281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "OVERPASS_URL = \"http://overpass-api.de/api/interpreter\"\n",
    "WIKIPEDIA_API_URL = \"https://ru.wikipedia.org/w/api.php\"\n",
    "WIKIDATA_API_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "def get_wikipedia_summary(title):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"query\", \"format\": \"json\", \"titles\": title,\n",
    "            \"prop\": \"extracts\", \"exintro\": True, \"explaintext\": True, \"redirects\": 1\n",
    "        }\n",
    "        response = requests.get(WIKIPEDIA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        page_id = next(iter(data['query']['pages']))\n",
    "        if page_id != '-1':\n",
    "            return data['query']['pages'][page_id].get('extract', '')\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def get_wikidata_description(qid):\n",
    "    try:\n",
    "        params = {\n",
    "            \"action\": \"wbgetentities\", \"format\": \"json\", \"ids\": qid,\n",
    "            \"props\": \"descriptions\", \"languages\": \"ru|en\"\n",
    "        }\n",
    "        response = requests.get(WIKIDATA_API_URL, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "        entity = data['entities'].get(qid)\n",
    "        if entity and 'descriptions' in entity:\n",
    "            if 'ru' in entity['descriptions']:\n",
    "                return entity['descriptions']['ru']['value']\n",
    "            elif 'en' in entity['descriptions']:\n",
    "                return entity['descriptions']['en']['value']\n",
    "        return \"\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"\"\n",
    "    except requests.exceptions.RequestException:\n",
    "        return \"\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def create_text_description(tags):\n",
    "    description_parts = []\n",
    "    if 'name' in tags:\n",
    "        description_parts.append(tags['name'])\n",
    "\n",
    "    tag_translations = {\n",
    "        'amenity': 'Тип', 'shop': 'Магазин', 'tourism': 'Туризм',\n",
    "        'leisure': 'Досуг', 'historic': 'Исторический объект', 'cuisine': 'Кухня'\n",
    "    }\n",
    "    for key, name in tag_translations.items():\n",
    "        if key in tags:\n",
    "            description_parts.append(f\"{name}: {tags[key].replace('_', ' ')}\")\n",
    "\n",
    "    if 'opening_hours' in tags: description_parts.append(f\"Часы работы: {tags['opening_hours']}\")\n",
    "    if 'wheelchair' in tags and tags['wheelchair'] == 'yes': description_parts.append(\"Доступно для инвалидных колясок\")\n",
    "    if 'internet_access' in tags and tags['internet_access'] == 'yes': description_parts.append(\"Есть доступ в интернет\")\n",
    "    if 'phone' in tags: description_parts.append(f\"Телефон: {tags['phone']}\")\n",
    "    if 'website' in tags: description_parts.append(f\"Вебсайт: {tags['website']}\")\n",
    "    if 'description' in tags: description_parts.append(f\"Описание: {tags['description']}\")\n",
    "\n",
    "    extra_description = \"\"\n",
    "    if 'wikipedia' in tags:\n",
    "        parts = tags['wikipedia'].split(':')\n",
    "        if len(parts) >= 2:\n",
    "            lang = parts[0]\n",
    "            title = parts[-1]\n",
    "            if lang == 'ru':\n",
    "                wiki_summary = get_wikipedia_summary(title)\n",
    "                if wiki_summary: extra_description = wiki_summary\n",
    "    elif 'wikidata' in tags:\n",
    "        wikidata_id = tags['wikidata']\n",
    "        wiki_description = get_wikidata_description(wikidata_id)\n",
    "        if wiki_description: extra_description = wiki_description\n",
    "\n",
    "    if extra_description: description_parts.append(extra_description)\n",
    "    return \". \".join(description_parts)\n",
    "\n",
    "def process_city_pois(city_name):\n",
    "    print(f\"\\n--- Начинаем обработку города: {city_name} ---\")\n",
    "\n",
    "    amenities_to_include = [\n",
    "        \"arts_centre\", \"theatre\", \"cinema\", \"museum\", \"library\", \"nightclub\", \"bar\",\n",
    "        \"restaurant\", \"cafe\", \"pub\", \"food_court\", \"marketplace\", \"fountain\",\n",
    "        \"public_bath\", \"sauna\", \"casino\", \"ferry_terminal\", \"attraction\",\n",
    "        \"theme_park\", \"water_park\", \"zoo\", \"aquarium\", \"planetarium\", \"gallery\",\n",
    "        \"viewpoint\", \"observatory\", \"place_of_worship\", \"community_centre\", \"social_facility\"\n",
    "    ]\n",
    "    shops_to_include = [\n",
    "        \"mall\", \"department_store\", \"books\", \"gift\", \"souvenir\", \"art\", \"antiques\",\n",
    "        \"craft\", \"boutique\", \"jewelry\", \"leather\", \"music\", \"shoes\", \"toys\", \"video\",\n",
    "        \"convenience\", \"supermarket\", \"bakery\", \"beverages\", \"confectionery\", \"deli\",\n",
    "        \"farm\", \"greengrocer\", \"ice_cream\", \"pastry\", \"wine\", \"stationery\", \"sports\",\n",
    "        \"fashion\", \"perfumery\"\n",
    "    ]\n",
    "\n",
    "    amenity_filter = \"|\".join(amenities_to_include)\n",
    "    shop_filter = \"|\".join(shops_to_include)\n",
    "\n",
    "    overpass_query_strict = f\"\"\"\n",
    "    [out:json][timeout:180];\n",
    "    area[name=\"{city_name}\"]->.searchArea;\n",
    "    (\n",
    "      node[\"tourism\"](area.searchArea);\n",
    "      way[\"tourism\"](area.searchArea);\n",
    "      relation[\"tourism\"](area.searchArea);\n",
    "\n",
    "      node[\"leisure\"](area.searchArea);\n",
    "      way[\"leisure\"](area.searchArea);\n",
    "      relation[\"leisure\"](area.searchArea);\n",
    "\n",
    "      node[\"historic\"](area.searchArea);\n",
    "      way[\"historic\"](area.searchArea);\n",
    "      relation[\"historic\"](area.searchArea);\n",
    "\n",
    "      node[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "      way[\"place\"~\"square|fountain\"](area.searchArea);\n",
    "\n",
    "      node[\"natural\"~\"park|wood|garden|beach|peak|water|forest|island|ridge|valley|volcano|wetland|glacier\"](area.searchArea);\n",
    "      way[\"natural\"~\"park|wood|garden|beach|peak|water|forest|island|ridge|valley|volcano|wetland|glacier\"](area.searchArea);\n",
    "      relation[\"natural\"~\"park|wood|garden|beach|peak|water|forest|island|ridge|valley|volcano|wetland|glacier\"](area.searchArea);\n",
    "\n",
    "      node[\"amenity\"~\"^{amenity_filter}$\"](area.searchArea);\n",
    "      way[\"amenity\"~\"^{amenity_filter}$\"](area.searchArea);\n",
    "\n",
    "      node[\"shop\"~\"^{shop_filter}$\"](area.searchArea);\n",
    "      way[\"shop\"~\"^{shop_filter}$\"](area.searchArea);\n",
    "\n",
    "      node[\"landuse\"~\"forest|park|recreation_ground|village_green\"](area.searchArea);\n",
    "      way[\"landuse\"~\"forest|park|recreation_ground|village_green\"](area.searchArea);\n",
    "    );\n",
    "    out body;\n",
    "    >;\n",
    "    out skel qt;\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"🚀 Отправляем фильтрованный запрос к Overpass API для {city_name}...\")\n",
    "    try:\n",
    "        response = requests.get(OVERPASS_URL, params={'data': overpass_query_strict}, timeout=180)\n",
    "        print(f\"✅ Запрос для {city_name} выполнен с кодом: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "        else:\n",
    "            print(f\"Ошибка выполнения запроса для {city_name}: {response.text}\")\n",
    "            data = {'elements': []}\n",
    "    except requests.exceptions.Timeout:\n",
    "        print(f\"Таймаут при запросе к Overpass API для {city_name}. Попробуйте увеличить timeout.\")\n",
    "        data = {'elements': []}\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Ошибка запроса к Overpass API для {city_name}: {e}\")\n",
    "        data = {'elements': []}\n",
    "\n",
    "    current_city_pois = []\n",
    "    print(f\"⚙️ Начинаем обработку {len(data['elements'])} объектов, полученных из Overpass для {city_name}...\")\n",
    "\n",
    "    for element in tqdm(data['elements'], desc=f\"Обработка и обогащение POI в {city_name}\"):\n",
    "        if 'tags' in element:\n",
    "            tags = element['tags']\n",
    "\n",
    "            if 'name' not in tags:\n",
    "                continue\n",
    "\n",
    "            lat, lon = (0, 0)\n",
    "            if element['type'] == 'node':\n",
    "                lat = element.get('lat')\n",
    "                lon = element.get('lon')\n",
    "            elif element['type'] == 'way' and 'center' in element:\n",
    "                lat = element['center'].get('lat')\n",
    "                lon = element['center'].get('lon')\n",
    "            elif element['type'] == 'relation' and 'center' in element:\n",
    "                lat = element['center'].get('lat')\n",
    "                lon = element['center'].get('lon')\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            if lat == 0 and lon == 0:\n",
    "                continue\n",
    "\n",
    "            text_description = create_text_description(tags)\n",
    "\n",
    "            current_city_pois.append({\n",
    "                'id': element['id'],\n",
    "                'type': element['type'],\n",
    "                'lat': lat,\n",
    "                'lon': lon,\n",
    "                'name': tags.get('name'),\n",
    "                'city': city_name,\n",
    "                'text_description': text_description,\n",
    "                'tags': tags\n",
    "            })\n",
    "\n",
    "        time.sleep(0.005)\n",
    "\n",
    "    print(f\"👍 Всего обработано и добавлено в список: {len(current_city_pois)} POI для {city_name}.\")\n",
    "\n",
    "    return current_city_pois\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    output_filename = 'Dataset/poi_dataset_russia_filtered_enriched.csv'\n",
    "\n",
    "    if os.path.exists(output_filename):\n",
    "        print(f\"Загружаем существующий датасет из '{output_filename}'...\")\n",
    "        existing_df = pd.read_csv(output_filename)\n",
    "        print(f\"Загружено {len(existing_df)} POI из существующего файла.\")\n",
    "    else:\n",
    "        print(f\"Файл '{output_filename}' не найден. Создадим новый датасет.\")\n",
    "        existing_df = pd.DataFrame()\n",
    "\n",
    "    cities_to_add = [\"Екатеринбург\"]\n",
    "\n",
    "    newly_collected_pois = []\n",
    "    for city in cities_to_add:\n",
    "        city_data = process_city_pois(city)\n",
    "        newly_collected_pois.extend(city_data)\n",
    "        time.sleep(10)\n",
    "\n",
    "    print(\"\\n--- Сбор данных для новых городов завершен! ---\")\n",
    "    print(f\"Всего собрано {len(newly_collected_pois)} новых POI.\")\n",
    "\n",
    "    combined_pois = pd.concat([existing_df, pd.DataFrame(newly_collected_pois)], ignore_index=True)\n",
    "\n",
    "    print(f\"Всего POI до удаления дубликатов: {len(combined_pois)}\")\n",
    "    combined_pois.dropna(subset=['name', 'lat', 'lon'], inplace=True)\n",
    "    combined_pois.drop_duplicates(subset=['id'], inplace=True)\n",
    "\n",
    "    print(f\"Итоговое количество уникальных и полных POI: {len(combined_pois)}\")\n",
    "\n",
    "    combined_pois.to_csv(output_filename, index=False)\n",
    "    print(f\"💾 Обновленный датасет успешно сохранен в '{output_filename}'\")"
   ],
   "id": "b39d2a53a961737b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаем существующий датасет из 'poi_dataset_russia_filtered_enriched.csv'...\n",
      "Загружено 46354 POI из существующего файла.\n",
      "\n",
      "--- Начинаем обработку города: Екатеринбург ---\n",
      "🚀 Отправляем фильтрованный запрос к Overpass API для Екатеринбург...\n",
      "✅ Запрос для Екатеринбург выполнен с кодом: 200\n",
      "⚙️ Начинаем обработку 173809 объектов, полученных из Overpass для Екатеринбург...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Обработка и обогащение POI в Екатеринбург: 100%|██████████| 173809/173809 [15:47<00:00, 183.35it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👍 Всего обработано и добавлено в список: 4109 POI для Екатеринбург.\n",
      "\n",
      "--- Сбор данных для новых городов завершен! ---\n",
      "Всего собрано 4109 новых POI.\n",
      "Всего POI до удаления дубликатов: 50463\n",
      "Итоговое количество уникальных и полных POI: 50463\n",
      "💾 Обновленный датасет успешно сохранен в 'poi_dataset_russia_filtered_enriched.csv'\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T13:04:24.841441Z",
     "start_time": "2025-06-22T13:04:09.440311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from dateutil import parser as dateparser\n",
    "\n",
    "df = pd.read_csv('Dataset/poi_dataset_russia_filtered_enriched.csv', dtype=str)\n",
    "df = df.drop_duplicates('id')\n",
    "\n",
    "tags_df = (\n",
    "    df['tags']\n",
    "    .apply(lambda x: ast.literal_eval(x) if pd.notnull(x) else {})\n",
    "    .apply(pd.Series)\n",
    ")\n",
    "tags_df = tags_df.rename(columns={c: c.replace(':','_').lower() for c in tags_df.columns})\n",
    "tags_df = tags_df.drop(columns=['name'], errors=True)\n",
    "\n",
    "tags_df['address'] = (\n",
    "    tags_df[['addr_street','addr_housenumber','addr_floor']]\n",
    "    .fillna('')\n",
    "    .agg(' '.join, axis=1)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "def normalize_phone(x):\n",
    "    if not isinstance(x, str) or not x.strip():\n",
    "        return None\n",
    "    digits = re.sub(r'\\D','',x)\n",
    "    if len(digits) == 10:\n",
    "        return '+7' + digits\n",
    "    if len(digits) == 11 and digits.startswith('8'):\n",
    "        return '+7' + digits[1:]\n",
    "    if len(digits) == 11 and digits.startswith('7'):\n",
    "        return '+' + digits\n",
    "    return None\n",
    "\n",
    "tags_df['phone_e164'] = (\n",
    "    tags_df.get('contact_phone','')\n",
    "    .combine_first(tags_df.get('phone',''))\n",
    "    .apply(normalize_phone)\n",
    ")\n",
    "\n",
    "def normalize_url(x):\n",
    "    if pd.isnull(x) or not x:\n",
    "        return None\n",
    "    return x if re.match(r'^https?://', x) else 'http://' + x\n",
    "\n",
    "tags_df['website'] = tags_df.get('contact_website','').apply(normalize_url)\n",
    "\n",
    "tags_df['check_date'] = tags_df.get('check_date','').apply(\n",
    "    lambda x: dateparser.parse(x).date().isoformat() if pd.notnull(x) and x else None\n",
    ")\n",
    "\n",
    "tags_df['opening_hours'] = (\n",
    "    tags_df.get('opening_hours','')\n",
    "    .str.replace(r'\\s*;\\s*','; ', regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "def normalize_wheelchair(x):\n",
    "    if pd.isnull(x):\n",
    "        return None\n",
    "    val = x.lower()\n",
    "    if val in ('yes','true'):\n",
    "        return True\n",
    "    if val in ('no','false'):\n",
    "        return False\n",
    "    return val\n",
    "\n",
    "tags_df['wheelchair'] = tags_df.get('wheelchair','').apply(normalize_wheelchair)\n",
    "\n",
    "category_cols = ['amenity','shop','tourism','historic','cuisine','memorial']\n",
    "tags_df['categories'] = tags_df[category_cols].apply(\n",
    "    lambda row: [v for v in row if pd.notnull(v) and v!=''], axis=1\n",
    ")\n",
    "\n",
    "df_clean = pd.concat([df.drop(columns=['tags']), tags_df], axis=1)\n",
    "\n",
    "df_clean['name'] = df_clean['name'].str.strip().str.title()\n",
    "if 'city' in df_clean.columns:\n",
    "    df_clean['city'] = df_clean['city'].str.strip().str.title()\n",
    "\n",
    "def clean_text(x):\n",
    "    if pd.isnull(x):\n",
    "        return ''\n",
    "    raw = BeautifulSoup(x, 'html.parser').get_text()\n",
    "    return re.sub(r'\\s+', ' ', raw).strip()\n",
    "\n",
    "df_clean['text_description'] = df_clean['text_description'].apply(clean_text)\n",
    "\n",
    "output_path = 'poi_dataset_cleaned.csv'\n",
    "df_clean.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned dataset saved to ./{output_path}\")\n"
   ],
   "id": "a16bcfd91bfc8dce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to ./poi_dataset_cleaned.csv\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-22T13:55:07.944413Z",
     "start_time": "2025-06-22T13:54:56.059443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=200,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "def enrich_poi_llama(description: str) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "You are a travel assistant. Given the following description in Russian, output a JSON object with exactly two keys:\n",
    "  \"summary\": one-sentence thematic summary like \"романтическое место для вечерней прогулки\",\n",
    "  \"themes\": list of 2–4 keywords describing main place types or themes (e.g. [\"музей\",\"парк\"]).\n",
    "\n",
    "Description:\n",
    "\\\"\\\"\\\"{description.strip()}\\\"\\\"\\\"\n",
    "\n",
    "Respond ONLY with the JSON object.\n",
    "\"\"\"\n",
    "    out = generator(prompt)[0][\"generated_text\"]\n",
    "    json_str = out[out.find(\"{\"): out.rfind(\"}\")+1]\n",
    "    return json.loads(json_str)\n",
    "\n",
    "df = pd.read_csv(\"poi_dataset_cleaned.csv\", dtype=str)\n",
    "summaries, themes = [], []\n",
    "\n",
    "for desc in tqdm(df[\"text_description\"], desc=\"Enriching with Llama2-7B 4bit\"):\n",
    "    if not isinstance(desc, str) or not desc.strip():\n",
    "        summaries.append(None)\n",
    "        themes.append([])\n",
    "        continue\n",
    "    try:\n",
    "        enriched = enrich_poi_llama(desc)\n",
    "        summaries.append(enriched.get(\"summary\"))\n",
    "        themes.append(enriched.get(\"themes\", []))\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        summaries.append(None)\n",
    "        themes.append([])\n",
    "\n",
    "df[\"summary\"] = summaries\n",
    "df[\"themes\"]  = themes\n",
    "df.to_csv(\"poi_dataset_enriched_local.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Done! Saved to poi_dataset_enriched_local.csv\")\n"
   ],
   "id": "3a65cbfb2fc79e9e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emil1\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# --- 1. Конфигурация 4-битной загрузки ---\u001B[39;00m\n\u001B[32m     12\u001B[39m MODEL_NAME = \u001B[33m\"\u001B[39m\u001B[33mmeta-llama/Llama-2-7b-chat-hf\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m bnb_config = \u001B[43mBitsAndBytesConfig\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mload_in_4bit\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbnb_4bit_quant_type\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mnf4\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbnb_4bit_use_double_quant\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[32m     17\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# --- 2. Загрузка токенизатора и модели с квантованием ---\u001B[39;00m\n\u001B[32m     20\u001B[39m tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\quantization_config.py:484\u001B[39m, in \u001B[36mBitsAndBytesConfig.__init__\u001B[39m\u001B[34m(self, load_in_8bit, load_in_4bit, llm_int8_threshold, llm_int8_skip_modules, llm_int8_enable_fp32_cpu_offload, llm_int8_has_fp16_weight, bnb_4bit_compute_dtype, bnb_4bit_quant_type, bnb_4bit_use_double_quant, bnb_4bit_quant_storage, **kwargs)\u001B[39m\n\u001B[32m    481\u001B[39m \u001B[38;5;28mself\u001B[39m.bnb_4bit_use_double_quant = bnb_4bit_use_double_quant\n\u001B[32m    483\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m bnb_4bit_compute_dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m484\u001B[39m     \u001B[38;5;28mself\u001B[39m.bnb_4bit_compute_dtype = \u001B[43mtorch\u001B[49m.float32\n\u001B[32m    485\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(bnb_4bit_compute_dtype, \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    486\u001B[39m     \u001B[38;5;28mself\u001B[39m.bnb_4bit_compute_dtype = \u001B[38;5;28mgetattr\u001B[39m(torch, bnb_4bit_compute_dtype)\n",
      "\u001B[31mNameError\u001B[39m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
